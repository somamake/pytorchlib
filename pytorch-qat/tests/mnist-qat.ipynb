{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0978f0eb-77b2-4b90-b8fc-fb059dda47b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import myobserver\n",
    "import myfake_quantize\n",
    "import mynet\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import torch.quantization as tq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e707dc-48fc-4a9b-a53c-e3fbd1e3386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layer):\n",
    "    if type(layer) == (nn.Linear or nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(layer.weight) # 重みを「一様分布のランダム値」に初期化\n",
    "        layer.bias.data.fill_(0.0)            # バイアスを「0」に初期化\n",
    "\n",
    "def train(model, device, train_loader, loss_func, optimizer):\n",
    "  total_acc = 0\n",
    "  total_loss = 0\n",
    "  model.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    output = model(data)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_func(output, target)\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad(): \n",
    "      pred = torch.argmax(output, dim = 1)\n",
    "      total_acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "  avg_acc = total_acc / len(train_loader.dataset)\n",
    "  avg_loss = total_loss / len(train_loader.dataset)\n",
    "  return avg_loss, avg_acc\n",
    "\n",
    "def test(model, device, test_loader, loss_func):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    ans_list = []\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += loss_func(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            total_acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "            ans_list += target.tolist()\n",
    "            pred_list += pred.tolist()\n",
    "    avg_acc = total_acc / len(test_loader.dataset)\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb4a2333-c8b8-473c-b611-c25c57d839f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerCh Affine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantNet(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): ApFusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=7, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): Pow2MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (conv1): ConvReLU2d(\n",
       "    1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (weight_fake_quant): ApFusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-4, quant_max=3, qscheme=torch.per_channel_affine, reduce_range=False\n",
       "      (activation_post_process): Pow2MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): ApFusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=7, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): Pow2MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (relu1): Identity()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): ConvReLU2d(\n",
       "    4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (weight_fake_quant): ApFusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-4, quant_max=3, qscheme=torch.per_channel_affine, reduce_range=False\n",
       "      (activation_post_process): Pow2MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): ApFusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=7, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): Pow2MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (relu2): Identity()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): LinearReLU(\n",
       "    in_features=392, out_features=32, bias=True\n",
       "    (weight_fake_quant): ApFusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-4, quant_max=3, qscheme=torch.per_channel_affine, reduce_range=False\n",
       "      (activation_post_process): Pow2MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): ApFusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=7, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): Pow2MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (relu3): Identity()\n",
       "  (fc2): Linear(\n",
       "    in_features=32, out_features=10, bias=True\n",
       "    (weight_fake_quant): ApFusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-4, quant_max=3, qscheme=torch.per_channel_affine, reduce_range=False\n",
       "      (activation_post_process): Pow2MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): ApFusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
       "      (activation_post_process): Pow2MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum,Flag, auto\n",
    "class FixedMode(Flag):\n",
    "    PerTen = auto()\n",
    "    PerCh = auto()\n",
    "    Pow2 = auto()\n",
    "    Float = auto()\n",
    "    Affine = auto()\n",
    "    Symmetric = auto()\n",
    "    PerTenPow2 = PerTen | Pow2 | Symmetric\n",
    "    PerChPow2 = PerCh | Pow2 | Symmetric\n",
    "    PerTenFloat = PerTen | Float | Symmetric\n",
    "    PerChFloat = PerCh | Float | Symmetric\n",
    "    PerTenPow2Affine = PerTen | Pow2 | Affine\n",
    "    PerChPow2Affine = PerCh | Pow2 | Affine\n",
    "    PerTenFloatAffine = PerTen | Float | Affine\n",
    "    PerChFloatAffine = PerCh | Float | Affine\n",
    "model_qat = mynet.QuantNet()\n",
    "model_qat.eval()\n",
    "act_bit = 3\n",
    "weight_bit = 3\n",
    "weight_qmin = -(2**(weight_bit - 1))\n",
    "weight_qmax = (2**(weight_bit - 1)-1)\n",
    "act_qmin = 0\n",
    "act_qmax = 2**act_bit - 1\n",
    "fixedmode = FixedMode.PerChPow2Affine\n",
    "act_qscheme = torch.per_channel_affine\n",
    "\n",
    "\n",
    "# weight_qscheme\n",
    "if (fixedmode & FixedMode.PerCh) and (fixedmode & FixedMode.Affine):\n",
    "    weight_qscheme=torch.per_channel_affine\n",
    "    print(\"PerCh Affine\")\n",
    "elif (fixedmode & FixedMode.PerCh) and (fixedmode & FixedMode.Symmetric):\n",
    "    weight_qscheme=torch.per_channel_symmetric\n",
    "    print(\"PerCh Symmetric\")\n",
    "elif (fixedmode & FixedMode.PerTen) and (fixedmode & FixedMode.Affine):\n",
    "    weight_qscheme=torch.per_tensor_affine\n",
    "    print(\"PerTen Affine\")\n",
    "elif (fixedmode & FixedMode.PerTen) and (fixedmode & FixedMode.Symmetric):\n",
    "    weight_qscheme=torch.per_tensor_symmetric\n",
    "    print(\"PerTen Symmetric\")\n",
    "\n",
    "fake_quantize = myfake_quantize.ApFusedMovingAvgObsFakeQuantize\n",
    "if (fixedmode & FixedMode.PerCh) and (fixedmode & FixedMode.Float):\n",
    "    act_observer = tq.MovingAverageMinMaxObserver\n",
    "    weight_observer = tq.MovingAveragePerChannelMinMaxObserver\n",
    "    \n",
    "elif (fixedmode & FixedMode.PerTen) and (fixedmode & FixedMode.Float):\n",
    "    act_observer = tq.MovingAverageMinMaxObserver\n",
    "    weight_observer = tq.MovingAverageMinMaxObserver\n",
    "    \n",
    "elif (fixedmode & FixedMode.PerCh) and (fixedmode & FixedMode.Pow2):\n",
    "    act_observer = myobserver.Pow2MovingAverageMinMaxObserver\n",
    "    weight_observer = myobserver.Pow2MovingAveragePerChannelMinMaxObserver\n",
    "    \n",
    "elif (fixedmode & FixedMode.PerTen) and (fixedmode & FixedMode.Pow2):\n",
    "    act_observer = myobserver.Pow2MovingAverageMinMaxObserver\n",
    "    weight_observer = myobserver.Pow2MovingAverageMinMaxObserver\n",
    "model_qat.qconfig = torch.quantization.QConfig(activation=fake_quantize.with_args(\n",
    "                                            observer=act_observer.with_args(quant_min=0,quant_max=act_qmax),dtype=torch.quint8,quant_min=0,quant_max=act_qmax),\n",
    "                      weight=fake_quantize.with_args(observer = weight_observer.with_args(quant_min=weight_qmin,quant_max=weight_qmax,qscheme = weight_qscheme)\n",
    "                                                     ,dtype=torch.qint8,quant_min=weight_qmin,quant_max=weight_qmax))\n",
    "model_qat.fc2.qconfig = torch.quantization.QConfig(activation=fake_quantize.with_args(\n",
    "                                            observer=act_observer.with_args(quant_min=0,quant_max=255),dtype=torch.quint8,quant_min=0,quant_max=255),\n",
    "                      weight=fake_quantize.with_args(observer = weight_observer.with_args(quant_min=weight_qmin,quant_max=weight_qmax,qscheme = weight_qscheme),dtype=torch.qint8,quant_min=weight_qmin,quant_max=weight_qmax))\n",
    "\n",
    "\n",
    "torch.quantization.fuse_modules(model_qat, [['conv1', 'relu1']],inplace=True)\n",
    "torch.quantization.fuse_modules(model_qat, [['conv2', 'relu2']],inplace=True)\n",
    "torch.quantization.fuse_modules(model_qat, [['fc1', 'relu3']],inplace=True)\n",
    "torch.quantization.prepare_qat(model_qat,inplace=True)\n",
    "# model_qat.conv1.activation_post_process.activation_post_process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d68ffada-0b00-4692-8b0f-8b4ef5a72924",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# データの読み出し方法の定義\n",
    "# 1stepの学習・テストごとに16枚ずつ画像を読みだす\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f46cfe51-171f-4e40-bfb1-436a3b22020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1/ 10] loss: 0.05358, acc: 0.76465 val_loss: 0.03317, val_acc: 0.84120\n",
      "[Epoch   2/ 10] loss: 0.02153, acc: 0.89123 val_loss: 0.02689, val_acc: 0.85640\n",
      "[Epoch   3/ 10] loss: 0.01705, acc: 0.91312 val_loss: 0.02582, val_acc: 0.86910\n",
      "[Epoch   4/ 10] loss: 0.01465, acc: 0.92585 val_loss: 0.02738, val_acc: 0.86110\n",
      "[Epoch   5/ 10] loss: 0.01263, acc: 0.93750 val_loss: 0.01806, val_acc: 0.90770\n",
      "[Epoch   6/ 10] loss: 0.01087, acc: 0.94463 val_loss: 0.01576, val_acc: 0.91760\n",
      "[Epoch   7/ 10] loss: 0.00992, acc: 0.95023 val_loss: 0.02234, val_acc: 0.88360\n",
      "[Epoch   8/ 10] loss: 0.00925, acc: 0.95423 val_loss: 0.01631, val_acc: 0.91700\n",
      "[Epoch   9/ 10] loss: 0.00864, acc: 0.95710 val_loss: 0.02429, val_acc: 0.87430\n",
      "[Epoch  10/ 10] loss: 0.00833, acc: 0.95905 val_loss: 0.02425, val_acc: 0.87800\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_qat.parameters(), lr=0.0001)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# QAT takes time and one needs to train over a few epochs.\n",
    "# Train and check accuracy after each epoch\n",
    "for epoch in range(1,epochs+1):\n",
    "    avg_loss, avg_acc = train(model_qat,device,trainloader,loss_func,optimizer)\n",
    "    if epoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        model_qat.apply(torch.quantization.disable_observer)\n",
    "    if epoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        model_qat.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    # Check the accuracy after each epoch\n",
    "    quantized_model = torch.quantization.convert(model_qat.eval(), inplace=False)\n",
    "    quantized_model.eval()\n",
    "    avg_val_loss, avg_val_acc = test(quantized_model,device,testloader,loss_func)\n",
    "    print(f'[Epoch {epoch:3d}/{epochs:3d}]' \\\n",
    "          f' loss: {avg_loss:.5f}, acc: {avg_acc:.5f}' \\\n",
    "          f' val_loss: {avg_val_loss:.5f}, val_acc: {avg_val_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1732a72-9f75-4404-8cf2-12655246edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8 = torch.quantization.convert(model_qat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20bf4cbb-42cb-4f39-8ae6-cdfbd45a55e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.2500],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8.conv2.weight().q_per_channel_scales()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9601d-fac7-4598-903e-257fd0e421db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "62fedd36b4de442daf001b244d21cd32eacbf369c33880d3a72aa48294f7f570"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
